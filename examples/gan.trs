use lin::Linear;
use reg::{BatchNorm1d};
use nonlin::{leaky_relu, tanh};

node Generator<noise -> image> {
    dim noise_dim = 100;
    dim image_dim = 28;
    tsr noise = [?, noise_dim];
    tsr image = [?, image_dim];
}

weights Generator<noise -> image> {
    lin1 = Linear::new(in=noise_dim, out=128);
    lin2 = Linear::new(in=128, out=256);
    bn1 = BatchNorm1d::new(num_features=256);
    lin3 = Linear::new(in=256, out=512);
    bn2 = BatchNorm1d::new(num_features=512);
    lin4 = Linear::new(in=512, out=1024);
    bn3 = BatchNorm1d::new(num_features=1024);
    lin5 = Linear::new(in=1024, out=image_dim);
}

graph Generator<noise -> image> {
    def new() -> Self {
        self
    }

    def forward {
        x
        |> lin1 |> leaky_relu(p=0.2)
        |> lin2 |> bn1 |> leaky_relu(p=0.2)
        |> lin3 |> bn2 |> leaky_relu(p=0.2)
        |> lin4 |> bn3 |> leaky_relu(p=0.2)
        |> lin5 |> tanh
    }
}

// class Generator(nn.Module):
//     def __init__(self):
//         super(Generator, self).__init__()
//
//         self.model = nn.Sequential(
//             nn.Linear(opt.latent_dim, 128),
//             nn.LeakyReLU(0.2, inplace=True),

//             nn.Linear(128, 256),
//             nn.BatchNorm1d(256),
//             nn.LeakyReLU(0.2, inplace=True),

//             nn.Linear(256, 512),
//             nn.BatchNorm1d(512),
//             nn.LeakyReLU(0.2, inplace=True),

//             nn.Linear(512, 1024),
//             nn.BatchNorm1d(1024),
//             nn.LeakyReLU(0.2, inplace=True),

//             nn.Linear(1024, opt.img_size**2),
//             nn.Tanh()
//         )
//
//     def forward(self, noise):
//         img = self.model(noise)
//         img = img.view(img.shape[0], opt.channels, opt.img_size, opt.img_size)
//         return img